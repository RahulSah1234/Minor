# -*- coding: utf-8 -*-
"""minor_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VLO8-aFMhHu3e9xW5cj9lllEPA83kGr0
"""

from google.colab import drive

drive.mount('/content/gdrive',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
import os
# import neural_structured_learning as nsl
import glob
import json
import numpy as np

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import  applications
from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import regularizers

from keras.models import load_model
from sklearn.metrics import confusion_matrix ,classification_report


import itertools
import os
import shutil
import random
import glob

pip install split-folders

import splitfolders
splitfolders.ratio("/content/gdrive/My Drive/output", output="data_minor_1", ratio=(.6, .2, .2), group_prefix=None, move=False)

train_path="/content/data_minor_1/train"

train_batches=ImageDataGenerator(zoom_range=1.5,rotation_range=90,horizontal_flip=True).flow_from_directory(directory=train_path,target_size= (224,224),batch_size=10,shuffle=True,class_mode='categorical')

val_path="/content/data_minor_1/val"

val_batches=ImageDataGenerator(zoom_range=1.5).flow_from_directory(directory=val_path,target_size= (224,224),batch_size=10,shuffle=True,class_mode='categorical')

test_path="/content/data_minor_1/test"

test_batches=ImageDataGenerator(zoom_range=1.5).flow_from_directory(directory=test_path,target_size= (224,224),batch_size=10,shuffle=True,class_mode='categorical')

# model = tf.keras.Sequential(
#     [
#      tf.keras.layers.Rescaling(1./255),
#      tf.keras.layers.Conv2D(32, 3, activation="relu"),
#      tf.keras.layers.MaxPooling2D(),
#      tf.keras.layers.Conv2D(32, 3, activation="relu"),
#      tf.keras.layers.MaxPooling2D(),
#      tf.keras.layers.Conv2D(32, 3, activation="relu"),
#      tf.keras.layers.MaxPooling2D(),
#      tf.keras.layers.Flatten(),
#      tf.keras.layers.Dense(128, activation="relu"),
#      tf.keras.layers.Dense(2)
#     ]
# )

# model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])

# model.fit(
#     train_batches,
#     validation_data = val_batches,
#     epochs = 50
# )

# model.evaluate(test_batches)

test_batches.class_indices

# for images,labels in test_batches:
#   classifications=model(images)

# prediction=model.predict(x=test_batches)

# prediction

# max_indices = np.argmax(prediction, axis=1)

# # Print the result
# print(max_indices)

# score=0
# for i in max_indices:
#   if i==1:
#    score+=-1
#   else:
#     score+=1
# print(score)



# import numpy

# plt.figure(figsize=(10,10))
# for images, labels in test_ds.take(1):
#   classifications = model(images)
#   # print(classifications)
  
#   for i in range(9):
#     ax = plt.subplot(3, 3, i + 1)
#     plt.imshow(images[i].numpy().astype("uint8"))
#     index = numpy.argmax(classifications[i])
#     plt.title("Pred: " + class_names[index] + " | Real: " + class_names[labels[i]])

# converter = tf.lite.TFLiteConverter.from_keras_model(model)
# tflite_model = converter.convert()

# with open("model.tflite", 'wb') as f:
#   f.write(tflite_model)

# import tensorflow as tf
# from tensorflow.keras.applications.densenet import DenseNet121
# from tensorflow.keras.layers import Dense, Flatten
# from tensorflow.keras.models import Model

# # Load the pre-trained model
# base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# # Freeze the pre-trained layers
# for layer in base_model.layers:
#     layer.trainable = False

# # Add a new output layer for binary classification
# x = base_model.output
# x = Flatten()(x)
# x = Dense(128, activation='relu')(x)
# predictions = Dense(2, activation='softmax')(x)

# # Create the new model
# model_de = Model(inputs=base_model.input, outputs=predictions)

# # Compile the model
# model_de.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# # Train the model on your data
# model_de.fit(
#     train_batches,
#     validation_data = val_batches,
#     epochs = 30
# )

import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model

# Load the pre-trained model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add a new output layer for classification
x = base_model.output
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(2, activation='softmax')(x)

# Create the new model
model_re = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model_re.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model on your data
model_re.fit(
    train_batches,
    validation_data = val_batches,
    epochs = 90
)

model_re.evaluate(test_batches)

prediction=model_re.predict(x=test_batches)

prediction

max_indices = np.argmax(prediction, axis=1)

# Print the result
print(max_indices)

score=0
for i in max_indices:
  if i==1:
   score+=-1
  else:
    score+=1
print(score)

"""# **Deploy**"""

tf.keras.models.save_model(model_re,'my_model3.hdf5')

# !pip install streamlit

# %%writefile app.py
# import streamlit as st
# st.write('# Hello World')
# st.write('## Run Streamlit on Colab without `ngrok` using `localtunnel`')

# !streamlit run app.py & npx localtunnel --port 8501

# # Import necessary libraries
# import streamlit as st
# import tensorflow as tf
# from PIL import Image
# import numpy as np

# # Load the pre-trained model
# model = tf.keras.models.load_model('/content/my_model2.hdf5')

# # Define the Streamlit app
# def app():
#     st.title('My Image Classification App')
#     # Define the user input
#     uploaded_file = st.file_uploader("Choose an image...", type="jpg")
#     if uploaded_file is not None:
#         # Load the image
#         image = Image.open(uploaded_file)
#         # Resize the image to the input size of the model
#         image = image.resize((224, 224))
#         # Convert the image to a numpy array
#         image_array = np.array(image)
#         # Preprocess the image
#         image_array = image_array / 255.0
#         image_array = np.expand_dims(image_array, axis=0)
#         # Make a prediction using the deep learning model
#         prediction = model.predict(image_array)
#         # Display the prediction
#         st.write('Prediction:', prediction)

# # Run the Streamlit app
# if __name__ == '__main__':
#     app()

